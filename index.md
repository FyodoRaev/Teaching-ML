![img](https://media.giphy.com/media/oNJ3am00JCroA/giphy.gif)

<h1 align="center">Welcome to the Machine Learning basics course.</h1>

<h2 align="left">Useful links: <li><a href="https://t.me/learningMll">Telegramm chat</a></li>
<li><a href="https://github.com/FyodoRaev/Teaching-ML">Github repo of the course</a></li>
<li><a href="https://arxiv.org/abs/2201.09746">Useful RL book</a></li>
<li><a href ="https://stanford.edu/~shervine/teaching/">Cheatsheets</a></li>
<li><a href ="https://discord.gg/Hj4ub9v5jE">Discord server</a></li>
</h2>

### Here will be records of lessons:
1. Intorduction to machine learning.Numpy :
 *  [Seminar and HW](https://github.com/FyodoRaev/Teaching-ML)
 *  [Useful link](https://www.youtube.com/c/joshstarmer/videos)
 *  [Numpy documentation](https://numpy.org/doc/stable/index.html)
2. Working with Data. 
*   [Homework and Lecture resourses](https://github.com/FyodoRaev/Teaching-ML/tree/main/Data%20wrangling)
*   [Example of EDA](https://www.kaggle.com/code/sdip28/pubg-exploratory-data-analysis-prediction/notebook)
*   [Lecture recording](https://youtu.be/ayDEICCgzLQ)
*   [Why should do we care about Feature-engineering and correlations?](explanation1.md)
3. Probability theory.
* [This](https://github.com/FyodoRaev/Teaching-ML/blob/main/probability_theory.md) is a link to the markdown with Probability theory session. I won't be able to have a call on this saturday so I just put all the neccesary references there. Just explore it. No homework on this topic. 
4. Linear regresion.
* [Notes](https://github.com/FyodoRaev/Teaching-ML/blob/main/What's%20inside%20.fit%20.predict%3F/ML_seminar_for_students.ipynb)
* Homework is in the README.md file
* [Great explanation of Gradient Descent](https://youtu.be/uJryes5Vk1o)
* Use [GeeksforGeeks](https://www.geeksforgeeks.org/machine-learning/?ref=shm), [Andrew Ng lessons](https://www.youtube.com/c/Deeplearningai) and [StatQuest](https://www.youtube.com/c/joshstarmer/videos) for getting DEEP understanding 
5. Overfitting problem.
* [Notes](https://github.com/FyodoRaev/Teaching-ML/blob/main/Overfitting/inclass.ipynb)
* [Homework](https://github.com/FyodoRaev/Teaching-ML/blob/main/Overfitting/hw.ipynb)
* Personal recommendation: "Deep Learning" by Ian Goodfellow et al. has a great explanation of this problem. 
6. Classification.
* [Notes](https://github.com/FyodoRaev/Teaching-ML/blob/main/Classification/classification_seminar.ipynb)
* [Homework](https://) #will add later ;(
* [Great explanation of LogLoss](https://youtu.be/HIQlmHxI6-0)
* [Another great explanation](https://youtu.be/SHEPb1JHw5o)
